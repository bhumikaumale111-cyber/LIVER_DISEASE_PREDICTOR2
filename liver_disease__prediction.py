# -*- coding: utf-8 -*-
"""Liver_Disease _Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DiDq1jlWHD_lBxoGE_7zMrijl7HGy8F1
"""

#importing required libraries for EDA

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Commented out IPython magic to ensure Python compatibility.
# %%writefile aap.py
# import streamlit as st
# import pandas as pd
# import numpy as np
# import pickle
# 
# # Load the pre-trained model, scaler, and label encoder
# @st.cache_resource
# def load_resources():
#     with open('best_liver_model.pkl', 'rb') as model_file:
#         model = pickle.load(model_file)
#     with open('scaler.pkl', 'rb') as scaler_file:
#         scaler = pickle.load(scaler_file)
#     with open('label_encoder.pkl', 'rb') as le_file:
#         label_encoder = pickle.load(le_file)
#     return model, scaler, label_encoder
# 
# model, scaler, label_encoder = load_resources()
# 
# st.title('Liver Disease Prediction App')
# st.write('Enter the patient details below to predict the likelihood of liver disease.')
# 
# # Input fields for user data
# age = st.slider('Age', 19, 77, 40)
# 
# sex_options = {'Female': 0, 'Male': 1}
# sex_display = st.selectbox('Sex', list(sex_options.keys()))
# sex = sex_options[sex_display]
# 
# albumin = st.number_input('Albumin (g/dL)', min_value=14.9, max_value=82.2, value=40.0, step=0.1)
# alkaline_phosphatase = st.number_input('Alkaline Phosphatase (U/L)', min_value=11.3, max_value=416.6, value=70.0, step=0.1)
# alanine_aminotransferase = st.number_input('Alanine Aminotransferase (U/L)', min_value=0.9, max_value=325.3, value=25.0, step=0.1)
# aspartate_aminotransferase = st.number_input('Aspartate Aminotransferase (U/L)', min_value=10.6, max_value=324.0, value=30.0, step=0.1)
# bilirubin = st.number_input('Bilirubin (mg/dL)', min_value=0.8, max_value=254.0, value=10.0, step=0.1)
# cholinesterase = st.number_input('Cholinesterase (U/L)', min_value=1.42, max_value=16.41, value=8.0, step=0.01)
# cholesterol = st.number_input('Cholesterol (mg/dL)', min_value=1.43, max_value=9.67, value=5.0, step=0.01)
# creatinina = st.number_input('Creatinine (mg/dL)', min_value=8.0, max_value=1079.1, value=80.0, step=0.1)
# gamma_glutamyl_transferase = st.number_input('Gamma-Glutamyl Transferase (U/L)', min_value=4.5, max_value=650.9, value=40.0, step=0.1)
# protein = st.number_input('Protein (g/dL)', min_value=44.8, max_value=90.0, value=70.0, step=0.1)
# 
# # Create a DataFrame from inputs
# input_data = pd.DataFrame([{
#     'age': age,
#     'sex': sex,
#     'albumin': albumin,
#     'alkaline_phosphatase': alkaline_phosphatase,
#     'alanine_aminotransferase': alanine_aminotransferase,
#     'aspartate_aminotransferase': aspartate_aminotransferase,
#     'bilirubin': bilirubin,
#     'cholinesterase': cholinesterase,
#     'cholesterol': cholesterol,
#     'creatinina': creatinina,
#     'gamma_glutamyl_transferase': gamma_glutamyl_transferase,
#     'protein': protein
# }])
# 
# # Order columns consistently with training data
# # (Assuming X_train had columns in this order after preprocessing in the notebook)
# ordered_columns = ['age', 'sex', 'albumin', 'alkaline_phosphatase',
#                    'alanine_aminotransferase', 'aspartate_aminotransferase', 'bilirubin',
#                    'cholinesterase', 'cholesterol', 'creatinina',
#                    'gamma_glutamyl_transferase', 'protein']
# 
# input_data = input_data[ordered_columns]
# 
# # Preprocess the input data
# # Scale numerical features
# scaled_input_data = scaler.transform(input_data)
# 
# # Make prediction
# if st.button('Predict'):
#     prediction_proba = model.predict_proba(scaled_input_data)[0]
#     prediction_class = model.predict(scaled_input_data)[0]
# 
#     # Decode the predicted class using the label encoder
#     predicted_category = label_encoder.inverse_transform([prediction_class])[0]
# 
#     st.subheader('Prediction Results:')
#     st.write(f"Predicted Liver Disease Category: **{predicted_category}**")
#     st.write("Prediction Probabilities:")
# 
#     # Display probabilities for all classes
#     class_names = label_encoder.inverse_transform(np.arange(len(label_encoder.classes_)))
#     for i, prob in enumerate(prediction_proba):
#         st.write(f"- {class_names[i]}: {prob:.2f}")
# 
#     st.markdown("--- Say something about the predicted category here. ---")

"""### Streamlit App Code

First, let's install `streamlit` if it's not already installed. Then, I'll provide the Python code for the Streamlit application. You can save this code as a `.py` file and run it using `streamlit run your_app_name.py`.
"""


import streamlit as st
import pandas as pd
import numpy as np
import pickle

# Load the pre-trained model, scaler, and label encoder
@st.cache_resource
def load_resources():
    with open('best_liver_model.pkl', 'rb') as model_file:
        model = pickle.load(model_file)
    with open('scaler.pkl', 'rb') as scaler_file:
        scaler = pickle.load(scaler_file)
    with open('label_encoder.pkl', 'rb') as le_file:
        label_encoder = pickle.load(le_file)
    return model, scaler, label_encoder

model, scaler, label_encoder = load_resources()

st.title('Liver Disease Prediction App')
st.write('Enter the patient details below to predict the likelihood of liver disease.')

# Input fields for user data
age = st.slider('Age', 19, 77, 40)

sex_options = {'Female': 0, 'Male': 1}
sex_display = st.selectbox('Sex', list(sex_options.keys()))
sex = sex_options[sex_display]

albumin = st.number_input('Albumin (g/dL)', min_value=14.9, max_value=82.2, value=40.0, step=0.1)
alkaline_phosphatase = st.number_input('Alkaline Phosphatase (U/L)', min_value=11.3, max_value=416.6, value=70.0, step=0.1)
alanine_aminotransferase = st.number_input('Alanine Aminotransferase (U/L)', min_value=0.9, max_value=325.3, value=25.0, step=0.1)
aspartate_aminotransferase = st.number_input('Aspartate Aminotransferase (U/L)', min_value=10.6, max_value=324.0, value=30.0, step=0.1)
bilirubin = st.number_input('Bilirubin (mg/dL)', min_value=0.8, max_value=254.0, value=10.0, step=0.1)
cholinesterase = st.number_input('Cholinesterase (U/L)', min_value=1.42, max_value=16.41, value=8.0, step=0.01)
cholesterol = st.number_input('Cholesterol (mg/dL)', min_value=1.43, max_value=9.67, value=5.0, step=0.01)
creatinina = st.number_input('Creatinine (mg/dL)', min_value=8.0, max_value=1079.1, value=80.0, step=0.1)
gamma_glutamyl_transferase = st.number_input('Gamma-Glutamyl Transferase (U/L)', min_value=4.5, max_value=650.9, value=40.0, step=0.1)
protein = st.number_input('Protein (g/dL)', min_value=44.8, max_value=90.0, value=70.0, step=0.1)

# Create a DataFrame from inputs
input_data = pd.DataFrame([{
    'age': age,
    'sex': sex,
    'albumin': albumin,
    'alkaline_phosphatase': alkaline_phosphatase,
    'alanine_aminotransferase': alanine_aminotransferase,
    'aspartate_aminotransferase': aspartate_aminotransferase,
    'bilirubin': bilirubin,
    'cholinesterase': cholinesterase,
    'cholesterol': cholesterol,
    'creatinina': creatinina,
    'gamma_glutamyl_transferase': gamma_glutamyl_transferase,
    'protein': protein
}])

# Order columns consistently with training data
# (Assuming X_train had columns in this order after preprocessing in the notebook)
ordered_columns = ['age', 'sex', 'albumin', 'alkaline_phosphatase',
                   'alanine_aminotransferase', 'aspartate_aminotransferase', 'bilirubin',
                   'cholinesterase', 'cholesterol', 'creatinina',
                   'gamma_glutamyl_transferase', 'protein']

input_data = input_data[ordered_columns]

# Preprocess the input data
# Scale numerical features
scaled_input_data = scaler.transform(input_data)

# Make prediction
if st.button('Predict'):
    prediction_proba = model.predict_proba(scaled_input_data)[0]
    prediction_class = model.predict(scaled_input_data)[0]

    # Decode the predicted class using the label encoder
    predicted_category = label_encoder.inverse_transform([prediction_class])[0]

    st.subheader('Prediction Results:')
    st.write(f"Predicted Liver Disease Category: **{predicted_category}**")
    st.write("Prediction Probabilities:")

    # Display probabilities for all classes
    class_names = label_encoder.inverse_transform(np.arange(len(label_encoder.classes_)))
    for i, prob in enumerate(prediction_proba):
        st.write(f"- {class_names[i]}: {prob:.2f}")

    st.markdown("--- Say something about the predicted category here. ---")

"""To run this Streamlit app:
1. Save the code from the previous cell into a file named `app.py` in your local environment.
2. Open your terminal or command prompt.
3. Navigate to the directory where you saved `app.py`.
4. Run the command: `streamlit run app.py`
5. Your Streamlit app will open in your web browser.
"""

#Loading dataset and treat 'NA' strings as missing values
data = pd.read_csv("project-data.csv", delimiter=';', skipinitialspace=True, na_values=['NA'])

"""##### DATA CLEANING"""

# Remove whitespace in columns and string values
data.columns = data.columns.str.strip()

for col in data.select_dtypes(include='object').columns:
    data[col] = data[col].str.strip()

#To view first 5 rows in the dataset
data.head()

#To view last 5 rows in the dataset
data.tail()

# Dataset info
data.info()

#after performing data.info()- we found that there are some missing values in the dataset.
#albumin, alkaline_phosphatase, alanine_aminotransferase,cholesterol have missing values.
#protein column is in object datatype and we need to change the datatypet to float.

# Convert 'protein' column to numeric, coercing errors to NaN
data['protein'] = pd.to_numeric(data['protein'], errors='coerce')

# Statistical summary
data.describe()

# Display Total no of rows & columns
data.shape

# Display column names
data.columns

#Check for missing values
data.isnull().sum()

# check for duplicates
data.duplicated().sum()

import warnings
warnings.filterwarnings("ignore")

#Handling missing values

data['albumin'].fillna(data['albumin'].median(), inplace=True)
data['alkaline_phosphatase'].fillna(data['alkaline_phosphatase'].median(), inplace=True)
data['protein'].fillna(data['protein'].median(), inplace=True)

# Display datatypes of each column
data.dtypes

"""##### Outliers Detection and Treatment"""

# Boxplot to detect outliers for numeric features
plt.figure(figsize=(12,8))
sns.boxplot(data=data.select_dtypes(include=['float64', 'int64']))
plt.title('Boxplot for Numeric Features')
plt.show()

# Outlier treatment using IQR capping
def cap_outliers_iqr(data):
    numeric_cols = data.select_dtypes(include=['float64', 'int64']).columns
    for col in numeric_cols:
        Q1 = data[col].quantile(0.25)
        Q3 = data[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        data[col] = np.where(data[col] < lower_bound, lower_bound, data[col])
        data[col] = np.where(data[col] > upper_bound, upper_bound, data[col])
    return data

df_capped = cap_outliers_iqr(data)

# Boxplot after treating outliers using IQR method for numeric features
plt.figure(figsize=(12,8))
sns.boxplot(data=data.select_dtypes(include=['float64', 'int64']))
plt.title('Boxplot for Numeric Features')
plt.show()

# Select numeric columns
numeric_cols = data.select_dtypes(include=['float64', 'int64']).columns

"""DATA VISUALIZATION"""

#Univariate analysis
# Histograms for single column 'age'
sns.histplot(data['age'], bins=30, kde=True)
plt.title('Age Distribution')
plt.show()

#HISTOGRAM
# Histograms for all numeric columns
data[numeric_cols].hist(figsize= (15,10), bins=15, facecolor="lavender", edgecolor="black")
plt.title("Histogram of numerical columns")
plt.show()

#univariate analysis for categorical column
sns.countplot(x='sex', data=data, color='lightgreen')
plt.title('Count of Passengers by Gender')
plt.show()

# Plot countplot for categorical features - bivariate
plt.figure(figsize=(10,6))
sns.countplot(data=data, x='category')
plt.title('Count of Samples per Category')
plt.show()

#Correlation Heatmap
plt.figure(figsize=(10,8))
corr=data[numeric_cols].corr()
sns.heatmap(corr, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation Heatmap of Numertical Variables")
plt.show()

"""High Positive Correlations (Strong Relationships)

Albumin - Protein (0.53)
Aspartate_Aminotransferase - Alanine_Aminotransferase (0.43)
Cholinesterase - Albumin (0.38)

- Most other pairs (e.g., Age, Bilirubin, Creatinina, Cholesterol, Gamma_glutamyl_transferase) have correlation values between -0.2 and +0.2, indicating weak or no linear relationship

##### FEATURE ENGINEERING

##### Encoding Techniques
"""

from sklearn.preprocessing import LabelEncoder

# Convert text categories into numeric labels
le = LabelEncoder()
data['sex'] = le.fit_transform(data['sex'])
data['category'] = le.fit_transform(data['category'])

# Save label encoder for deployment, we can inverse_transform predictions later
# pickle.dump(le, open('label_encoder.pkl', 'wb'))

"""##### Train Test Split"""

from sklearn.model_selection import train_test_split, GridSearchCV

#Split into features(X) and Target(y)
#category is our target column

X =data.drop("category", axis=1)
y = data["category"]

# Split into train-test sets using 80-20 split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)

print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

#StandardScaler
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)    # fit on train only
X_test = scaler.transform(X_test)          # apply same transform to test

"""Standard Scaler - Transforms data to have a mean of 0 and a standard deviation of 1."""

import numpy as np

print(np.isnan(X_train).sum())
print(np.isnan(X_test).sum())

# For training data

columns = ['age', 'sex', 'albumin', 'alkaline_phosphatase', 'alanine_aminotransferase',
                'aspartate_aminotransferase', 'bilirubin', 'cholinesterase', 'cholesterol',
                'creatinina', 'gamma_glutamyl_transferase', 'protein']

# Convert NumPy X_train and X_test back to DataFrame
X_train_df = pd.DataFrame(X_train, columns=columns)
X_test_df = pd.DataFrame(X_test, columns=columns)

# missing value handling:
for col in X_train_df.columns:
    if X_train_df[col].isnull().sum() > 0:
        median_val = X_train_df[col].median()
        X_train_df[col].fillna(median_val, inplace=True)

for col in X_test_df.columns:
    if X_test_df[col].isnull().sum() > 0:
        # Use training median to avoid data leakage
        median_val = X_train_df[col].median()
        X_test_df[col].fillna(median_val, inplace=True)

print(np.isnan(X_train).sum())
print(np.isnan(X_test).sum())

"""#### MODEL BUILDING

##### LOGISTIC REGRESSION - MODEL 1
"""

from sklearn.linear_model import LogisticRegression

#fitting the model
lr=LogisticRegression(max_iter=200, random_state=42)
lr.fit(X_train, y_train)

lr_ypred_test = lr.predict(X_test)
lr_yproba_test = lr.predict_proba(X_test)[:,1]

lr_ypred_train = lr.predict(X_train)

from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, accuracy_score, classification_report, roc_curve, roc_auc_score

"""##### Evaluation:"""

print('Classification Report of Training Data')
print(f'{classification_report(y_train, lr_ypred_train)}')
print("Accuracy: ", accuracy_score(y_train, lr_ypred_train))
print("precision: ", precision_score(y_train, lr_ypred_train, average='weighted'))
print("Recall: ", recall_score(y_train, lr_ypred_train, average='weighted'))
print("F1-Score: ", f1_score(y_train, lr_ypred_train, average='weighted'))

print('\nClassification Report of Test Data')
print(f'{classification_report(y_test, lr_ypred_test)}')
print("Accuracy: ", accuracy_score(y_test, lr_ypred_test))
print("precision: ", precision_score(y_test, lr_ypred_test, average='weighted'))
print("Recall: ", recall_score(y_test, lr_ypred_test, average='weighted'))
print("F1-Score: ", f1_score(y_test, lr_ypred_test, average='weighted'))

cm = confusion_matrix(y_test, lr_ypred_test)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix for Logistic Regression')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

"""Confusion matrix is a table that evaluates the performance of a classification model by comparing its predictions to the actual outcomes"""

# binary encoding
y_test_binary = (y_test == y_test.unique()[1]).astype(int)

# ROC
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

fpr, tpr, thresholds = roc_curve(y_test_binary, lr_yproba_test)
auc_score = roc_auc_score(y_test_binary, lr_yproba_test)

plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, color='blue', label=f'Logistic Regression (AUC = {auc_score:.2f})')
plt.plot([0, 1], [0, 1], color='red', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Logistic Regression')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()



"""##### DESCISION TREE - MODEL 2"""

from sklearn.tree import  DecisionTreeClassifier
from sklearn import tree

"""using Grid Search CV to automatically find the best combination of hyperparameters for my model.
It tests multiple parameter values and selects the one that gives the best accuracy with cross-validation,
ensuring the model is well-optimized and performs consistently
"""

dt = DecisionTreeClassifier(random_state=42)
param_grid_dt = {
    'max_depth': [5, 10, 20, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 5],
    'criterion': ['gini', 'entropy']
}

grid_search_dt = GridSearchCV(dt, param_grid_dt, cv=5, scoring='accuracy', n_jobs=-1)
grid_search_dt.fit(X_train, y_train)

print("Decision Tree Best Params:", grid_search_dt.best_params_)
print("Decision Tree Best Score:", grid_search_dt.best_score_)

best_dt = grid_search_dt.best_estimator_

dt_ypred_test = best_dt.predict(X_test)

dt_ypred_train = best_dt.predict(X_train)

print('Classification Report of Training Data')
print(f'{classification_report(y_train, dt_ypred_train)}')
print("Accuracy: ", accuracy_score(y_train, dt_ypred_train))
print("precision: ", precision_score(y_train, dt_ypred_train, average='weighted'))
print("Recall: ", recall_score(y_train, dt_ypred_train, average='weighted'))
print("F1-Score: ", f1_score(y_train, dt_ypred_train, average='weighted'))

print('\nClassification Report of Test Data')
print(f'{classification_report(y_test, dt_ypred_test)}')
print("Accuracy: ", accuracy_score(y_test, dt_ypred_test))
print("precision: ", precision_score(y_test, dt_ypred_test, average='weighted'))
print("Recall: ", recall_score(y_test, dt_ypred_test, average='weighted'))
print("F1-Score: ", f1_score(y_test, dt_ypred_test, average='weighted'))

# Compute the confusion matrix
from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix
cm = confusion_matrix(y_test, dt_ypred_test)

# Visualize the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_dt.classes_)
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix - Desicion Tree')
plt.show()

dt_yproba_test = best_dt.predict_proba(X_test)[:, 1]

#ROC Curve and auc

fpr, tpr, thresholds = roc_curve(y_test_binary, dt_yproba_test)
auc_score = roc_auc_score(y_test_binary, dt_yproba_test)

# Plot ROC Curve
plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, color='blue', label=f'Decision Tree (AUC = {auc_score:.2f})')
plt.plot([0, 1], [0, 1], color='red', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Decision Tree')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()





"""##### RANDOM FOREST - MODEL 3"""

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=10, min_samples_leaf=5, random_state=42)
rf.fit(X_train, y_train)

rf_ypred_train = rf.predict(X_train)
rf_ypred_test = rf.predict(X_test)

print('ðŸ”¹ Random Forest Classification Report - Training Data')
print(classification_report(y_train, rf_ypred_train))
print("Accuracy: ", accuracy_score(y_train, rf_ypred_train))
print("Precision: ", precision_score(y_train, rf_ypred_train, average='weighted'))
print("Recall: ", recall_score(y_train, rf_ypred_train, average='weighted'))
print("F1-Score: ", f1_score(y_train, rf_ypred_train, average='weighted'))

# Evaluation - Test Data
print('\nðŸ”¹ Random Forest Classification Report - Test Data')
print(classification_report(y_test, rf_ypred_test))
print("Accuracy: ", accuracy_score(y_test, rf_ypred_test))
print("Precision: ", precision_score(y_test, rf_ypred_test, average='weighted'))
print("Recall: ", recall_score(y_test, rf_ypred_test, average='weighted'))
print("F1-Score: ", f1_score(y_test, rf_ypred_test, average='weighted'))

cm = confusion_matrix(y_test, rf_ypred_test)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix for Random Forest')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

rf_yproba_test = rf.predict_proba(X_test)[:, 1]

#ROC Curve and auc

fpr, tpr, thresholds = roc_curve(y_test_binary, rf_yproba_test)
auc_score = roc_auc_score(y_test_binary, rf_yproba_test)

# Plot ROC Curve
plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, color='blue', label=f'Random Forest (AUC = {auc_score:.2f})')
plt.plot([0, 1], [0, 1], color='red', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Random Forest')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

"""##### GRADIENT BOOSTING - MODEL 4"""

from sklearn.ensemble import GradientBoostingClassifier

gbc = GradientBoostingClassifier(random_state=42)
param_grid_gbc = {
    'n_estimators': [50, 100, 150],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7],
    'subsample': [0.8, 1.0]
}

grid_search_gbc = GridSearchCV(gbc, param_grid_gbc, cv=5, scoring='accuracy', n_jobs=-1)
grid_search_gbc.fit(X_train, y_train)

best_gb = grid_search_gbc.best_estimator_
print("Gradient Boosting Best Params:", grid_search_gbc.best_params_)
print("Gradient Boosting Best Score:", grid_search_gbc.best_score_)

gb_ypred_train = best_gb.predict(X_train)
gb_ypred_test = best_gb.predict(X_test)

print('Classification Report of Training Data')
print(f'{classification_report(y_train, gb_ypred_train)}')
print("Accuracy: ", accuracy_score(y_train, gb_ypred_train))
print("precision: ", precision_score(y_train, gb_ypred_train, average='weighted'))
print("Recall: ", recall_score(y_train, gb_ypred_train, average='weighted'))
print("F1-Score: ", f1_score(y_train, gb_ypred_train, average='weighted'))

print('\nClassification Report of Test Data')
print(f'{classification_report(y_test, gb_ypred_test)}')
print("Accuracy: ", accuracy_score(y_test, gb_ypred_test))
print("precision: ", precision_score(y_test, gb_ypred_test, average='weighted'))
print("Recall: ", recall_score(y_test, gb_ypred_test, average='weighted'))
print("F1-Score: ", f1_score(y_test, gb_ypred_test, average='weighted'))

# Compute the confusion matrix
cm = confusion_matrix(y_test, gb_ypred_test)

# Visualize the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_gb.classes_)
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix - Gradient Boosting')
plt.show()

gb_yproba_test = best_gb.predict_proba(X_test)[:, 1]

#ROC Curve and auc

fpr, tpr, thresholds = roc_curve(y_test_binary, gb_yproba_test)
auc_score = roc_auc_score(y_test_binary, gb_yproba_test)

# Plot ROC Curve
plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, color='blue', label=f'Gradient boosting (AUC = {auc_score:.2f})')
plt.plot([0, 1], [0, 1], color='red', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Gradient Boosting')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()



"""##### SUPPORT VECTOR MACHINE (SVM) - MODEL 5"""

from sklearn.svm import SVC

svc = SVC(probability = True, random_state=42)
param_grid_svc = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf'],
    'gamma': ['scale', 'auto']
}

gridsearch_svc = GridSearchCV(svc, param_grid_svc, cv=5, scoring='accuracy', n_jobs=-1)
gridsearch_svc.fit(X_train, y_train)

best_svc = gridsearch_svc.best_estimator_

print("SVC Best Params:", gridsearch_svc.best_params_)
print("SVC Best Score:", gridsearch_svc.best_score_)

svc_ypred_train = best_svc.predict(X_train)
svc_ypred_test = best_svc.predict(X_test)

print('Classification Report of Training Data')
print(f'{classification_report(y_train, svc_ypred_train)}')
print("Accuracy: ", accuracy_score(y_train, svc_ypred_train))
print("precision: ", precision_score(y_train, svc_ypred_train, average='weighted'))
print("Recall: ", recall_score(y_train, svc_ypred_train, average='weighted'))
print("F1-Score: ", f1_score(y_train, svc_ypred_train, average='weighted'))

print('\nClassification Report of Test Data')
print(f'{classification_report(y_test, svc_ypred_test)}')
print("Accuracy: ", accuracy_score(y_test, svc_ypred_test))
print("precision: ", precision_score(y_test, svc_ypred_test, average='weighted'))
print("Recall: ", recall_score(y_test, svc_ypred_test, average='weighted'))
print("F1-Score: ", f1_score(y_test, svc_ypred_test, average='weighted'))

# Compute the confusion matrix
cm = confusion_matrix(y_test, svc_ypred_test)

# Visualize the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_svc.classes_)
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix - Support Vector Machine')
plt.show()

svc_yproba_test = best_svc.predict_proba(X_test)[:, 1]

#ROC Curve and auc

fpr, tpr, thresholds = roc_curve(y_test_binary, svc_yproba_test)
auc_score = roc_auc_score(y_test_binary, svc_yproba_test)

# Plot ROC Curve
plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, color='blue', label=f'Support Vector Machine (AUC = {auc_score:.2f})')
plt.plot([0, 1], [0, 1], color='red', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - SVM')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()



"""##### K Nearest Neighbors (KNN) - MODEL 6"""

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier()
param_grid_knn = {
    'n_neighbors': [3, 5, 7],
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan']
}

gridsearch_knn = GridSearchCV(knn, param_grid_knn, cv=5, scoring='accuracy', n_jobs=-1)
gridsearch_knn.fit(X_train, y_train)

best_knn = gridsearch_knn.best_estimator_

print("KNN Best Params:", gridsearch_knn.best_params_)
print("KNN Best Score:", gridsearch_knn.best_score_)

knn_ypred_train = best_knn.predict(X_train)
knn_ypred_test = best_knn.predict(X_test)

print('Classification Report of Training Data')
print(f'{classification_report(y_train, knn_ypred_train)}')
print("Accuracy: ", accuracy_score(y_train, knn_ypred_train))
print("precision: ", precision_score(y_train, knn_ypred_train, average='weighted'))
print("Recall: ", recall_score(y_train, knn_ypred_train, average='weighted'))
print("F1-Score: ", f1_score(y_train, knn_ypred_train, average='weighted'))

print('\nClassification Report of Test Data')
print(f'{classification_report(y_test, knn_ypred_test)}')
print("Accuracy: ", accuracy_score(y_test, knn_ypred_test))
print("precision: ", precision_score(y_test, knn_ypred_test, average='weighted'))
print("Recall: ", recall_score(y_test, knn_ypred_test, average='weighted'))
print("F1-Score: ", f1_score(y_test, knn_ypred_test, average='weighted'))

# Compute the confusion matrix
cm = confusion_matrix(y_test, knn_ypred_test)

# Visualize the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_knn.classes_)
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix - KNN')
plt.show()

knn_yproba_test = best_knn.predict_proba(X_test)[:, 1]

#ROC Curve and auc

fpr, tpr, thresholds = roc_curve(y_test_binary, knn_yproba_test)
auc_score = roc_auc_score(y_test_binary, knn_yproba_test)

# Plot ROC Curve
plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, color='blue', label=f'KNN (AUC = {auc_score:.2f})')
plt.plot([0, 1], [0, 1], color='red', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - KNN')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

"""##### Gaussian Naive Bayes (GridSearch) - Model 7"""

from sklearn.naive_bayes import GaussianNB

gnb = GaussianNB()
param_grid_gnb = {
    'var_smoothing': [1e-9, 1e-8, 1e-7]
}

gridsearch_gnb = GridSearchCV(gnb, param_grid_gnb, cv=5, scoring='accuracy', n_jobs=-1)
gridsearch_gnb.fit(X_train, y_train)

best_gnb = gridsearch_gnb.best_estimator_

print("Naive Bayes Best Params:", gridsearch_gnb.best_params_)
print("Naive Bayes Best Score:", gridsearch_gnb.best_score_)

gnb_ypred_train = best_gnb.predict(X_train)
gnb_ypred_test = best_gnb.predict(X_test)

print('Classification Report of Training Data')
print(f'{classification_report(y_train, gnb_ypred_train)}')
print("Accuracy: ", accuracy_score(y_train, gnb_ypred_train))
print("precision: ", precision_score(y_train, gnb_ypred_train, average='weighted'))
print("Recall: ", recall_score(y_train, gnb_ypred_train, average='weighted'))
print("F1-Score: ", f1_score(y_train, gnb_ypred_train, average='weighted'))

print('\nClassification Report of Test Data')
print(f'{classification_report(y_test, gnb_ypred_test)}')
print("Accuracy: ", accuracy_score(y_test, gnb_ypred_test))
print("precision: ", precision_score(y_test, gnb_ypred_test, average='weighted'))
print("Recall: ", recall_score(y_test, gnb_ypred_test, average='weighted'))
print("F1-Score: ", f1_score(y_test, gnb_ypred_test, average='weighted'))

# Compute the confusion matrix
cm = confusion_matrix(y_test, gnb_ypred_test)

# Visualize the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_gnb.classes_)
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix - Gaussian Naive Bayes')
plt.show()

gnb_yproba_test = best_gnb.predict_proba(X_test)[:, 1]

#ROC Curve and auc

fpr, tpr, thresholds = roc_curve(y_test_binary, gnb_yproba_test)
auc_score = roc_auc_score(y_test_binary, gnb_yproba_test)

# Plot ROC Curve
plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, color='blue', label=f'Gussian Naive Bayes  (AUC = {auc_score:.2f})')
plt.plot([0, 1], [0, 1], color='red', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Gussian Naive Bayes')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()



"""##### SUMMARY CHART"""

# Model names
models = ['Logistic Regression', 'Decision Tree', 'Random Forest', 'Gradient Boosting', 'SVM', 'KNN', 'Naive Bayes']

# Evaluation metrics
train_accuracy = [0.96, 0.96, 0.96, 1.00, 0.98, 0.96, 0.93]
test_accuracy = [0.91, 0.85, 0.83, 0.86, 0.88, 0.85, 0.87]
test_weighted_f1 = [0.90, 0.83, 0.78, 0.84, 0.87, 0.80, 0.87]

# Bar width and positions
bar_width = 0.25
indices = np.arange(len(models))

plt.figure(figsize=(12, 6))

# Plot bars
bars1 = plt.bar(indices, train_accuracy, width=bar_width, label='Train Accuracy', color='skyblue')
bars2 = plt.bar(indices + bar_width, test_accuracy, width=bar_width, label='Test Accuracy', color='lightgreen')
bars3 = plt.bar(indices + 2*bar_width, test_weighted_f1, width=bar_width, label='Test Weighted F1-Score', color='salmon')

#Adding value labels on top of bars
for bar in bars1:
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,
             f"{bar.get_height():.2f}", ha='center', va='bottom', fontsize=9)

for bar in bars2:
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,
             f"{bar.get_height():.2f}", ha='center', va='bottom', fontsize=9)

for bar in bars3:
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,
             f"{bar.get_height():.2f}", ha='center', va='bottom', fontsize=9)

# Labels and title
plt.xlabel('Models')
plt.ylabel('Scores')
plt.title('Model Comparison: Training and Test Metrics')
plt.xticks(indices + bar_width, models, rotation=30, ha='right')
plt.ylim([0.7, 1.10])
plt.legend()
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()

plt.show()



"""The model with the best test data performance is considered the best.

Training data metrics help diagnose overfitting or underfitting but do not determine model quality by themselves.

A model that performs well on training data but drops on test data is likely overfitting.

##### By comparing all the models "LOGISTIC REGRESSION" is the best model.

Best Model: Logistic Regression

- Best Test Accuracy:
It achieved the highest test accuracy (91%), which means it performed best on unseen data.

â€” High F1-score (0.90):
The F1-score combines both precision and recall, showing that it balanced correct positive predictions and minimized false results.

â€” No Overfitting:
Even though some models (like Gradient Boosting) reached 100% training accuracy, their test accuracy dropped to 86%, indicating slight overfitting.
Logistic Regressionâ€™s training accuracy (96%) and test accuracy (91%) are close, which means it generalizes very well.

â€” Simplicity & Interpretability:
Itâ€™s also easier to interpret â€” each featureâ€™s coefficient shows its impact on the disease prediction.

| Model                   | Train Accuracy | Test Accuracy | Weighted F1 | Remarks                           |
| ----------------------- | -------------- | ------------- | ----------- | --------------------------------- |
| Logistic Regression     | 0.97           | 0.91          | 0.90        | Best balance, generalizes well    |
| SVM                     | 0.98           | 0.88          | 0.87        | Strong but slightly less accurate |
| Random Forest           | 0.96           | 0.83          | 0.78        | Overfitting (train >> test)       |
| Gradient Boosting       | 1.00           | 0.86          | 0.84        | Overfitted                        |
| Decision Tree           | 0.96           | 0.85          | 0.83        | Overfitting and variance          |
| KNN                     | 0.96           | 0.85          | 0.80        | Sensitive to data scaling         |
| Naive Bayes             | 0.93           | 0.87          | 0.87        | Performs decently but less stable |
"""



"""Saving the best model"""

import pickle
best_model = lr  # selecting the model after reviewing metrics

pickle.dump(best_model, open('best_liver_model.pkl', 'wb'))
pickle.dump(scaler, open('scaler.pkl', 'wb'))
pickle.dump(le, open('label_encoder.pkl', 'wb'))
print("Best model and preprocessing objects saved successfully for Streamlit deployment.")



